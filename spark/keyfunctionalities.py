# #
# # from pyspark import SparkContext, SparkConf
# #
# # conf = SparkConf().setAppName("IPCount").setMaster("local[*]")
# # sc = SparkContext(conf=conf)
# #
# # data=[ "168.182.0.1.1","168.182.0.1.2","168.182.0.1.1","168.182.0.1.3",
# #  "168.182.0.1.2","168.182.0.1.1","168.182.0.1.3","168.182.0.1.4",
# #  "168.182.0.1.3","168.182.0.1.1","168.182.0.1.3","168.182.0.1.2",
# #  "168.182.0.1.4","168.182.0.1.1","168.182.0.1.2","168.182.0.1.1"
# # ]
# #
# # rdd =sc.parallelize(data)
# # print(rdd.collect())
# #
# # pair_rdd=rdd.map(lambda x: (x,1))
# #
# # reduce_key=pair_rdd.reduceByKey(lambda x,y:x+y)
# #
#
# from pyspark import SparkConf, SparkContext
#
# # 1Ô∏è‚É£ Create Spark configuration
# conf = SparkConf() \
#     .setAppName("IPAnalysis") \
#     .setMaster("local[*]")
#
# # 2Ô∏è‚É£ Create SparkContext
# sc = SparkContext(conf=conf)
#
# # 3Ô∏è‚É£ Input data
# data = [
#  "168.182.0.1.1","168.182.0.1.2","168.182.0.1.1","168.182.0.1.3",
#  "168.182.0.1.2","168.182.0.1.1","168.182.0.1.3","168.182.0.1.4",
#  "168.182.0.1.3","168.182.0.1.1","168.182.0.1.3","168.182.0.1.2",
#  "168.182.0.1.4","168.182.0.1.1","168.182.0.1.2","168.182.0.1.1"
# ]
#
# # 4Ô∏è‚É£ Create RDD
# rdd = sc.parallelize(data)
#
# # 5Ô∏è‚É£ Convert to (ip, 1)
# pair_rdd = rdd.map(lambda x: (x, 1))
#
# # 6Ô∏è‚É£ reduceByKey ‚Üí count frequency
# count_rdd = pair_rdd.reduceByKey(lambda a, b: a + b)
#
# # 7Ô∏è‚É£ sortByKey ‚Üí sort by IP
# sorted_by_key = count_rdd.sortByKey()
#
# # 8Ô∏è‚É£ sortBy ‚Üí sort by count descending
# sorted_by_count = count_rdd.sortBy(lambda x: x[1], ascending=False)
#
# # 9Ô∏è‚É£ groupByKey (not recommended but shown)
# grouped = pair_rdd.groupByKey().mapValues(sum)
#
# # üîü Second highest IP
# second_highest = sorted_by_count.take(2)[1]
#
# # üîç Output
# print("Counts:", count_rdd.collect())
# print("Sorted by count:", sorted_by_count.collect())
# print("Second highest IP:", second_highest)
#
# # 1Ô∏è‚É£1Ô∏è‚É£ Stop Spark
# sc.stop()



from pyspark import SparkContext

sc = SparkContext(master="local[*]", appName="SecondHighestIP")

# Input data
data = [
    "168.182.0.1.1","168.182.0.1.2","168.182.0.1.1","168.182.0.1.3",
    "168.182.0.1.2","168.182.0.1.1","168.182.0.1.3","168.182.0.1.4",
    "168.182.0.1.3","168.182.0.1.1","168.182.0.1.3","168.182.0.1.2",
    "168.182.0.1.4","168.182.0.1.1","168.182.0.1.2","168.182.0.1.1"
]

# Create RDD
rdd = sc.parallelize(data)

# ----------------------------
# 1. map (IP, 1)
# ----------------------------
mapped_rdd = rdd.map(lambda ip: (ip, 1))

# ----------------------------
# 2. reduceByKey (count per IP)
# ----------------------------
count_rdd = mapped_rdd.reduceByKey(lambda a, b: a + b)

# ----------------------------
# 3. groupByKey (for learning purpose)
# ----------------------------
grouped_rdd = mapped_rdd.groupByKey() \
    .mapValues(lambda values: sum(values))

# ----------------------------
# 4. sortByKey (alphabetical IP)
# ----------------------------
sorted_by_key = count_rdd.sortByKey()

# ----------------------------
# 5. sortBy (count descending)
# ----------------------------
sorted_by_count = count_rdd.sortBy(lambda x: x[1], ascending=False)

# ----------------------------
# 6. Find SECOND HIGHEST
# ----------------------------
second_highest = sorted_by_count.take(2)[1]

# ----------------------------
# Print results
# ----------------------------
print("IP Count using reduceByKey:")
print(count_rdd.collect())

print("\nIP Count using groupByKey:")
print(grouped_rdd.collect())

print("\nSorted by IP (sortByKey):")
print(sorted_by_key.collect())

print("\nSorted by Count Desc (sortBy):")
print(sorted_by_count.collect())

print("\nSecond Highest IP:")
print(second_highest)

sc.stop()
